# Arquitetura de Dados - Conta Azul
## Proposta de SoluÃ§Ã£o para Analytics Engineer | Google Cloud Platform

### VisÃ£o Geral

Esta proposta apresenta uma arquitetura moderna de dados baseada em princÃ­pios de **Data Mesh** e **Medallion Architecture**, desenhada para suportar as trÃªs frentes de dados da Conta Azul: Product Analytics, Data Science e Revenue Operations.

A soluÃ§Ã£o utiliza **Google Cloud Platform (GCP)** pela sua integraÃ§Ã£o nativa entre serviÃ§os, facilidade de uso e custo-benefÃ­cio competitivo.

### PrincÃ­pios Norteadores

1. **Simplicidade**: SoluÃ§Ãµes pragmÃ¡ticas com serviÃ§os gerenciados do GCP
2. **Escalabilidade**: Crescimento horizontal sem reengenharia (serverless-first)
3. **Custo-efetividade**: Pague apenas pelo uso, otimizaÃ§Ã£o automÃ¡tica
4. **Manutenibilidade**: CÃ³digo versionado, testado e documentado
5. **Self-Service**: Autonomia para analistas e cientistas de dados

---

## Ãndice

1. [Arquitetura Proposta](#arquitetura-proposta)
2. [Stack TecnolÃ³gica GCP](#stack-tecnolÃ³gica-gcp)
3. [Camadas de Dados](#camadas-de-dados)
4. [EstratÃ©gia de IngestÃ£o](#estratÃ©gia-de-ingestÃ£o)
5. [TransformaÃ§Ã£o com Dataform](#transformaÃ§Ã£o-com-dataform)
6. [OrquestraÃ§Ã£o com Cloud Composer](#orquestraÃ§Ã£o-com-cloud-composer)
7. [Qualidade e GovernanÃ§a](#qualidade-e-governanÃ§a)
8. [Camada de Consumo](#camada-de-consumo)
9. [Estimativa de Custos](#estimativa-de-custos)
10. [Roadmap de ImplementaÃ§Ã£o](#roadmap-de-implementaÃ§Ã£o)

---

## Arquitetura Proposta

### Diagrama de Arquitetura (High-Level)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FONTES DE DADOS                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PostgreSQL   â”‚  Firebase    â”‚  Salesforce  â”‚  Google Analytics 4         â”‚
â”‚ (Transacional)â”‚  (Eventos)   â”‚  (CRM)       â”‚  (Product Analytics)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚              â”‚              â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  INGESTÃƒO      â”‚
              â”‚                â”‚
              â”‚ â€¢ Fivetran     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ â€¢ Datastream   â”‚        â”‚
              â”‚ â€¢ Cloud Run    â”‚        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                      â”‚                 â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚         CLOUD STORAGE (Data Lake)                â”‚
       â”‚                                                  â”‚
       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
       â”‚  â”‚  BRONZE LAYER (Raw Data)                â”‚    â”‚
       â”‚  â”‚  â€¢ Formato: Avro/Parquet                â”‚    â”‚
       â”‚  â”‚  â€¢ Particionado por data ingestÃ£o       â”‚    â”‚
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
       â”‚                 â”‚                                â”‚
       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
       â”‚  â”‚  SILVER LAYER (Cleaned & Conformed)     â”‚    â”‚
       â”‚  â”‚  â€¢ Dados limpos e padronizados          â”‚    â”‚
       â”‚  â”‚  â€¢ Type casting e validaÃ§Ãµes            â”‚    â”‚
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
       â”‚                 â”‚                                â”‚
       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
       â”‚  â”‚  GOLD LAYER (Business Ready)            â”‚    â”‚
       â”‚  â”‚  â€¢ Modelos dimensionais                 â”‚    â”‚
       â”‚  â”‚  â€¢ AgregaÃ§Ãµes e mÃ©tricas                â”‚    â”‚
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚        BIGQUERY (Data Warehouse)           â”‚
       â”‚                                            â”‚
       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
       â”‚  â”‚ Product        â”‚  â”‚ Revenue Ops      â”‚ â”‚
       â”‚  â”‚ Analytics      â”‚  â”‚ (RevOps)         â”‚ â”‚
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
       â”‚  â”‚ Data Science (Feature Store)       â”‚   â”‚
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Looker /     â”‚  â”‚ Vertex AIâ”‚  â”‚ Data Catalog   â”‚
â”‚  Looker Studioâ”‚  â”‚ Notebooksâ”‚  â”‚ (GovernanÃ§a)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    (Analistas)    (Cientistas)   (Todos)
```

---

## Stack TecnolÃ³gica GCP

### Camada de Armazenamento
- **Cloud Storage**: Data Lake (Bronze/Silver/Gold layers)
- **BigQuery**: Data Warehouse serverless e analytics engine
- **Dataplex**: GovernanÃ§a, catalogaÃ§Ã£o e gerenciamento de metadados

### Camada de IngestÃ£o
- **Fivetran** (managed) ou **Airbyte** (open-source): Conectores ELT
- **Datastream**: CDC (Change Data Capture) para PostgreSQL/MySQL
- **Cloud Functions / Cloud Run**: IngestÃ£o customizada (APIs, webhooks)
- **BigQuery Data Transfer Service**: Google Marketing Platform, SaaS apps

### Camada de TransformaÃ§Ã£o
- **Dataform**: TransformaÃ§Ãµes SQL declarativas (gerenciado pelo Google)
- **BigQuery Scripting**: Processos complexos em SQL procedural
- **Dataproc Serverless**: Spark jobs para transformaÃ§Ãµes pesadas (opcional)

### Camada de OrquestraÃ§Ã£o
- **Cloud Composer (Apache Airflow gerenciado)**: OrquestraÃ§Ã£o de pipelines
- **Cloud Scheduler**: Triggers simples e agendamentos

### Camada de Consumo
- **Looker** ou **Looker Studio**: Business Intelligence
- **Vertex AI Workbench**: Notebooks para Data Science
- **BigQuery ML**: Modelos de ML dentro do BigQuery

### GovernanÃ§a e Qualidade
- **Dataplex Data Quality**: ValidaÃ§Ãµes automÃ¡ticas de dados
- **dbt Tests** (via Dataform): Testes de integridade
- **Cloud Monitoring & Logging**: Observabilidade
- **Cloud Data Loss Prevention (DLP)**: ProteÃ§Ã£o de dados sensÃ­veis (PII)

---

### Por que Dataform ao invÃ©s de dbt?

**Vantagens do Dataform no GCP**:
1. **Nativo do BigQuery**: Gerenciado pelo Google, zero infraestrutura
2. **IntegraÃ§Ã£o perfeita**: Git nativo, CI/CD built-in, IAM do GCP
3. **Custo zero de licenÃ§a**: IncluÃ­do no BigQuery (vs dbt Cloud pago)
4. **Performance**: Otimizado para BigQuery (query compilation)
5. **Sintaxe similar ao dbt**: MigraÃ§Ã£o fÃ¡cil se necessÃ¡rio

**Trade-offs**:
- Ecossistema menor que dbt (menos packages community)
- DocumentaÃ§Ã£o menos madura
- Menor adoÃ§Ã£o no mercado (mas crescendo rÃ¡pido)

**DecisÃ£o**: Usar **Dataform** pela integraÃ§Ã£o nativa e custo-benefÃ­cio, mas manter compatibilidade conceitual com dbt (caso precise migrar futuramente).

---

## Camadas de Dados (Medallion Architecture)

### Bronze Layer (Raw Data)
**LocalizaÃ§Ã£o**: `gs://conta-azul-datalake/bronze/`

**CaracterÃ­sticas**:
- Dados brutos, sem transformaÃ§Ã£o (ELT - Extract, Load, Transform)
- Formato: **Parquet** com compressÃ£o Snappy (custo-benefÃ­cio)
- Particionamento: `/year=YYYY/month=MM/day=DD/` (otimizaÃ§Ã£o de query)
- ImutÃ¡vel (append-only) - auditoria e reprocessamento
- RetenÃ§Ã£o: 2 anos (Lifecycle management automÃ¡tico)
- BigLake Tables: Consulta direta do BigQuery sem ingestÃ£o

**Estrutura de pastas**:
```
gs://conta-azul-datalake/bronze/
â”œâ”€â”€ sources/
â”‚   â”œâ”€â”€ postgres_production/
â”‚   â”‚   â”œâ”€â”€ customers/
â”‚   â”‚   â”‚   â””â”€â”€ year=2025/month=10/day=10/data.parquet
â”‚   â”‚   â”œâ”€â”€ invoices/
â”‚   â”‚   â”œâ”€â”€ payments/
â”‚   â”‚   â””â”€â”€ subscriptions/
â”‚   â”œâ”€â”€ firebase_analytics/
â”‚   â”‚   â””â”€â”€ events/
â”‚   â”œâ”€â”€ salesforce_crm/
â”‚   â”‚   â”œâ”€â”€ leads/
â”‚   â”‚   â”œâ”€â”€ opportunities/
â”‚   â”‚   â””â”€â”€ accounts/
â”‚   â””â”€â”€ google_analytics_4/
â”‚       â””â”€â”€ events/
â””â”€â”€ _metadata/
    â””â”€â”€ ingestion_logs.json
```

**Exemplo de External Table (BigLake)**:
```sql
-- Permite query direto do Cloud Storage sem copiar dados
CREATE EXTERNAL TABLE `bronze.postgres_customers`
WITH PARTITION COLUMNS (
  year INT64,
  month INT64,
  day INT64
)
OPTIONS (
  format = 'PARQUET',
  uris = ['gs://conta-azul-datalake/bronze/sources/postgres_production/customers/*'],
  hive_partition_uri_prefix = 'gs://conta-azul-datalake/bronze/sources/postgres_production/customers/',
  require_hive_partition_filter = TRUE
);
```

### Silver Layer (Cleaned & Conformed)
**LocalizaÃ§Ã£o**: `BigQuery dataset: silver_*` + backup em `gs://conta-azul-datalake/silver/`

**CaracterÃ­sticas**:
- Dados limpos, padronizados e deduplificados
- Type casting correto (STRING â†’ DATE, TIMESTAMP, NUMERIC)
- NormalizaÃ§Ã£o de nomes (snake_case)
- RemoÃ§Ã£o de duplicatas (QUALIFY ROW_NUMBER())
- Enriquecimento com metadados (_loaded_at, _source_system)
- Particionado por data de evento/transaÃ§Ã£o
- Clustering por colunas de filtro comum

**Estrutura BigQuery**:
```
Project: conta-azul-prod
â”œâ”€â”€ Dataset: silver_core
â”‚   â”œâ”€â”€ customers (partitioned by created_date, clustered by customer_segment)
â”‚   â”œâ”€â”€ transactions
â”‚   â””â”€â”€ products
â”œâ”€â”€ Dataset: silver_product_analytics
â”‚   â”œâ”€â”€ events (partitioned by event_date, clustered by event_name, user_id)
â”‚   â””â”€â”€ sessions
â””â”€â”€ Dataset: silver_crm
    â”œâ”€â”€ leads
    â””â”€â”€ opportunities
```

**Exemplo de tabela Silver**:
```sql
-- ConfiguraÃ§Ã£o da tabela (Dataform)
-- definitions/silver/core/customers.sqlx

config {
  type: "incremental",
  schema: "silver_core",
  uniqueKey: ["customer_id"],
  bigquery: {
    partitionBy: "created_date",
    clusterBy: ["customer_segment", "country"]
  },
  tags: ["daily", "core"]
}

WITH source AS (
  SELECT * FROM ${ref("bronze", "postgres_customers")}
  ${when(incremental(), `WHERE updated_at > (SELECT MAX(updated_at) FROM ${self()})`)}
),

cleaned AS (
  SELECT
    id AS customer_id,
    LOWER(TRIM(email)) AS email,
    company_name,
    CASE
      WHEN segment IN ('PME', 'MEI', 'AutÃ´nomo') THEN segment
      ELSE 'Outros'
    END AS customer_segment,
    CAST(created_at AS DATE) AS created_date,
    created_at,
    updated_at,
    'postgres_production' AS _source_system,
    CURRENT_TIMESTAMP() AS _loaded_at
  FROM source
  WHERE email IS NOT NULL
    AND created_at IS NOT NULL
    -- DeduplicaÃ§Ã£o (pega versÃ£o mais recente)
  QUALIFY ROW_NUMBER() OVER (PARTITION BY id ORDER BY updated_at DESC) = 1
)

SELECT * FROM cleaned
```

### Gold Layer (Business Ready)
**LocalizaÃ§Ã£o**: `BigQuery datasets: gold_*` (por domÃ­nio de negÃ³cio)

**CaracterÃ­sticas**:
- Modelos dimensionais (Star Schema / Kimball)
- AgregaÃ§Ãµes prÃ©-computadas
- MÃ©tricas de negÃ³cio calculadas
- SLAs definidos (fresshness, latÃªncia)
- Otimizado para consumo (BI, Data Science)
- DocumentaÃ§Ã£o rica (business glossary)

**Estrutura por Data Mesh Domains**:
```
Project: conta-azul-prod
â”œâ”€â”€ Dataset: gold_product_analytics (Owner: Product Analytics Team)
â”‚   â”œâ”€â”€ fct_user_engagement
â”‚   â”œâ”€â”€ fct_feature_adoption
â”‚   â”œâ”€â”€ dim_users
â”‚   â””â”€â”€ dim_features
â”œâ”€â”€ Dataset: gold_revenue_ops (Owner: RevOps Team)
â”‚   â”œâ”€â”€ fct_monthly_recurring_revenue
â”‚   â”œâ”€â”€ fct_sales_pipeline
â”‚   â”œâ”€â”€ fct_customer_health
â”‚   â”œâ”€â”€ dim_customers
â”‚   â””â”€â”€ dim_subscription_plans
â””â”€â”€ Dataset: gold_data_science (Owner: Data Science Team)
    â”œâ”€â”€ features_customer_churn
    â”œâ”€â”€ features_revenue_forecast
    â””â”€â”€ training_datasets_*
```

**Exemplo de Fact Table (Gold)**:
```sql
-- definitions/gold/revenue_ops/fct_monthly_recurring_revenue.sqlx

config {
  type: "table",
  schema: "gold_revenue_ops",
  description: "Monthly Recurring Revenue por mÃªs e segmento de cliente",
  bigquery: {
    partitionBy: "month",
    clusterBy: ["customer_segment"]
  },
  tags: ["daily", "revenue_ops", "critical"]
}

WITH subscriptions AS (
  SELECT * FROM ${ref("silver_core", "subscriptions")}
  WHERE status = 'active'
),

customers AS (
  SELECT * FROM ${ref("silver_core", "customers")}
),

aggregated AS (
  SELECT
    DATE_TRUNC(s.start_date, MONTH) AS month,
    c.customer_segment,
    c.country,
    COUNT(DISTINCT s.subscription_id) AS active_subscriptions,
    SUM(s.monthly_value_brl) AS mrr_brl,
    SUM(s.annual_value_brl) / 12 AS arr_normalized_brl,
    AVG(s.monthly_value_brl) AS avg_ticket_brl
  FROM subscriptions s
  INNER JOIN customers c USING (customer_id)
  GROUP BY 1, 2, 3
)

SELECT
  *,
  -- CÃ¡lculo de growth MoM
  mrr_brl - LAG(mrr_brl) OVER (
    PARTITION BY customer_segment
    ORDER BY month
  ) AS mrr_growth_brl,
  CURRENT_TIMESTAMP() AS _updated_at
FROM aggregated
```

---

## EstratÃ©gia de IngestÃ£o

### 1. Dados Transacionais (PostgreSQL, MySQL)

**OpÃ§Ã£o A: Datastream (CDC em tempo real)**
- **Quando usar**: Dados crÃ­ticos que precisam de baixa latÃªncia (<5 min)
- **Como funciona**: Captura mudanÃ§as no banco via CDC (Change Data Capture)
- **Destino**: Cloud Storage (Bronze) â†’ BigQuery (automÃ¡tico)

**ConfiguraÃ§Ã£o exemplo**:
```yaml
# Datastream connection profile
source:
  type: postgresql
  hostname: prod-db.conta-azul.com.br
  port: 5432
  database: production
  username: datastream_user

destination:
  cloud_storage:
    bucket: gs://conta-azul-datalake/bronze/postgres_production/
    format: AVRO
    partition: daily

tables:
  - customers
  - invoices
  - payments
  - subscriptions

options:
  cdc_strategy: auto  # Log-based CDC
  backfill: true       # Full load inicial + CDC contÃ­nuo
```

**OpÃ§Ã£o B: Fivetran (Managed ELT)**
- **Quando usar**: MÃºltiplas fontes SaaS (Salesforce, HubSpot, etc.)
- **Vantagem**: Zero manutenÃ§Ã£o, 400+ conectores
- **Custo**: ~$1.20 por MAR (Monthly Active Row)

**OpÃ§Ã£o C: Airbyte (Open-Source)**
- **Quando usar**: ReduÃ§Ã£o de custo, controle total
- **Deploy**: Cloud Run (container serverless) ou GKE
- **Custo**: Apenas infraestrutura (~$50-100/mÃªs)

### 2. Eventos de Produto (Firebase, GA4, Custom Events)

**Firebase â†’ BigQuery Export (Nativo)**:
```
Firebase Analytics
    â†“ (streaming, sem cÃ³digo)
BigQuery: analytics_XXXXXX.events_YYYYMMDD
    â†“
Dataform: normalizaÃ§Ã£o para silver_product_analytics.events
```

**Google Analytics 4 â†’ BigQuery**:
- Export diÃ¡rio automÃ¡tico (sem custo adicional)
- Tabela particionada: `analytics_XXXXXX.events_YYYYMMDD`

**Custom Events via Cloud Pub/Sub**:
```javascript
// Frontend/Backend envia evento
const {PubSub} = require('@google-cloud/pubsub');
const pubsub = new PubSub();

await pubsub.topic('user-events').publish(
  Buffer.from(JSON.stringify({
    event_name: 'invoice_created',
    user_id: '12345',
    properties: {
      invoice_value: 150.00,
      due_date: '2025-11-10'
    },
    timestamp: new Date().toISOString()
  }))
);
```

**Pub/Sub â†’ Cloud Storage â†’ BigQuery**:
```
Cloud Pub/Sub Topic
    â†“
Dataflow (Streaming)
    â†“
Cloud Storage (Bronze - buffer 5 min)
    â†“
BigQuery (BigLake External Table)
```

### 3. APIs Externas e SaaS (Salesforce, HubSpot, Stripe)

**Fivetran (recomendado para simplicidade)**:
```
Conectores:
- Salesforce (leads, opportunities, accounts)
- HubSpot (contacts, deals)
- Stripe (payments, customers, invoices)

Schedule: Daily at 3 AM BRT
Destination: BigQuery bronze_* datasets
```

**Alternativa - Cloud Run + Cloud Scheduler (custom)**:
```python
# cloud_run/salesforce_extractor/main.py
from simple_salesforce import Salesforce
from google.cloud import storage
import pandas as pd

def extract_salesforce(request):
    sf = Salesforce(username=..., password=..., security_token=...)

    # Query leads modificados nas Ãºltimas 24h
    query = """
        SELECT Id, Name, Email, Status, CreatedDate
        FROM Lead
        WHERE LastModifiedDate >= YESTERDAY
    """
    data = sf.query_all(query)

    # Salvar no Cloud Storage (Bronze)
    df = pd.DataFrame(data['records'])
    blob = storage.Client().bucket('conta-azul-datalake').blob(
        f'bronze/salesforce_crm/leads/year={year}/month={month}/day={day}/data.parquet'
    )
    blob.upload_from_string(df.to_parquet(), content_type='application/octet-stream')

    return {'status': 'success', 'records': len(df)}
```

### 4. Arquivos Manuais (CSV, Excel)

**Self-Service Upload**:
```
1. Analista faz upload para gs://conta-azul-datalake/uploads/
2. Cloud Function detecta novo arquivo (trigger)
3. Valida schema e qualidade bÃ¡sica
4. Move para Bronze com metadados
5. Notifica no Slack: "Arquivo X disponÃ­vel em bronze.manual_uploads"
```

---

## TransformaÃ§Ã£o com Dataform

### Estrutura do RepositÃ³rio

```
dataform/
â”œâ”€â”€ definitions/              # Modelos SQL
â”‚   â”œâ”€â”€ sources/             # DeclaraÃ§Ã£o de fontes Bronze
â”‚   â”‚   â””â”€â”€ sources.js
â”‚   â”œâ”€â”€ staging/             # Silver layer (1:1 com sources)
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_postgres__customers.sqlx
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_postgres__invoices.sqlx
â”‚   â”‚   â”‚   â””â”€â”€ stg_postgres__payments.sqlx
â”‚   â”‚   â”œâ”€â”€ product_analytics/
â”‚   â”‚   â”‚   â””â”€â”€ stg_firebase__events.sqlx
â”‚   â”‚   â””â”€â”€ crm/
â”‚   â”‚       â””â”€â”€ stg_salesforce__leads.sqlx
â”‚   â”œâ”€â”€ intermediate/        # LÃ³gica reutilizÃ¡vel
â”‚   â”‚   â”œâ”€â”€ int_customer_metrics.sqlx
â”‚   â”‚   â””â”€â”€ int_invoice_aggregations.sqlx
â”‚   â””â”€â”€ marts/               # Gold layer (consumo final)
â”‚       â”œâ”€â”€ product_analytics/
â”‚       â”‚   â”œâ”€â”€ fct_user_engagement.sqlx
â”‚       â”‚   â””â”€â”€ dim_users.sqlx
â”‚       â”œâ”€â”€ revenue_ops/
â”‚       â”‚   â”œâ”€â”€ fct_monthly_recurring_revenue.sqlx
â”‚       â”‚   â””â”€â”€ fct_customer_health.sqlx
â”‚       â””â”€â”€ data_science/
â”‚           â””â”€â”€ features_customer_churn.sqlx
â”œâ”€â”€ includes/                # FunÃ§Ãµes reutilizÃ¡veis (JS)
â”‚   â”œâ”€â”€ constants.js
â”‚   â””â”€â”€ helpers.js
â”œâ”€â”€ tests/                   # Testes customizados
â”‚   â””â”€â”€ assert_revenue_positive.sqlx
â”œâ”€â”€ dataform.json            # ConfiguraÃ§Ã£o do projeto
â””â”€â”€ package.json
```

### Exemplo de Modelo Staging (Silver)

```sql
-- definitions/staging/core/stg_postgres__customers.sqlx

config {
  type: "incremental",
  schema: "silver_core",
  uniqueKey: ["customer_id"],
  description: "Customers limpos e normalizados do PostgreSQL",
  bigquery: {
    partitionBy: "created_date",
    clusterBy: ["customer_segment", "country"]
  },
  tags: ["daily", "core"],

  // Assertions (testes inline)
  assertions: {
    uniqueKey: ["customer_id"],
    nonNull: ["customer_id", "email", "created_date"]
  }
}

-- DependÃªncias
${ref("bronze_postgres", "customers")}

-- LÃ³gica de ingestÃ£o incremental
pre_operations {
  ${when(incremental(),
    `DELETE FROM ${self()} WHERE customer_id IN (
      SELECT DISTINCT id FROM ${ref("bronze_postgres", "customers")}
      WHERE updated_at > (SELECT MAX(updated_at) FROM ${self()})
    )`
  )}
}

-- TransformaÃ§Ã£o
WITH source AS (
  SELECT * FROM ${ref("bronze_postgres", "customers")}
  ${when(incremental(),
    `WHERE updated_at > (SELECT MAX(updated_at) FROM ${self()})`
  )}
),

cleaned AS (
  SELECT
    -- PKs e IDs
    id AS customer_id,

    -- Dados pessoais (normalizados)
    LOWER(TRIM(email)) AS email,
    INITCAP(TRIM(company_name)) AS company_name,
    REGEXP_REPLACE(phone, r'[^0-9]', '') AS phone_clean,

    -- ClassificaÃ§Ãµes
    CASE
      WHEN segment IN ('PME', 'MEI', 'AutÃ´nomo') THEN segment
      ELSE 'Outros'
    END AS customer_segment,

    COALESCE(country, 'BR') AS country,

    -- Timestamps
    CAST(created_at AS DATE) AS created_date,
    created_at,
    updated_at,

    -- Metadados
    'postgres_production' AS _source_system,
    CURRENT_TIMESTAMP() AS _loaded_at

  FROM source

  -- Filtros de qualidade
  WHERE email IS NOT NULL
    AND created_at IS NOT NULL
    AND id IS NOT NULL

  -- DeduplicaÃ§Ã£o (Ãºltima versÃ£o)
  QUALIFY ROW_NUMBER() OVER (
    PARTITION BY id
    ORDER BY updated_at DESC
  ) = 1
)

SELECT * FROM cleaned

-- Post-hook: registrar estatÃ­sticas
post_operations {
  INSERT INTO silver_core._table_metadata (table_name, row_count, updated_at)
  VALUES ('customers', (SELECT COUNT(*) FROM ${self()}), CURRENT_TIMESTAMP())
}
```

### Exemplo de Modelo Intermediate

```sql
-- definitions/intermediate/int_customer_lifetime_metrics.sqlx

config {
  type: "view",  // View (nÃ£o materializada) - recalcula sempre
  schema: "silver_core",
  description: "MÃ©tricas agregadas de lifetime do customer",
  tags: ["intermediate"]
}

WITH customers AS (
  SELECT * FROM ${ref("stg_postgres__customers")}
),

invoices AS (
  SELECT * FROM ${ref("stg_postgres__invoices")}
  WHERE status IN ('paid', 'pending')
),

payments AS (
  SELECT * FROM ${ref("stg_postgres__payments")}
  WHERE payment_status = 'confirmed'
),

customer_metrics AS (
  SELECT
    c.customer_id,
    c.email,
    c.customer_segment,
    c.created_date,

    -- MÃ©tricas de receita
    COUNT(DISTINCT i.invoice_id) AS total_invoices,
    SUM(i.total_value_brl) AS total_revenue_brl,
    AVG(i.total_value_brl) AS avg_invoice_value_brl,
    MAX(i.invoice_date) AS last_invoice_date,

    -- MÃ©tricas de pagamento
    COUNT(DISTINCT p.payment_id) AS total_payments,
    SUM(CASE WHEN p.payment_method = 'credit_card' THEN 1 ELSE 0 END) AS credit_card_payments,

    -- MÃ©tricas de tempo
    DATE_DIFF(CURRENT_DATE(), c.created_date, DAY) AS customer_age_days,
    DATE_DIFF(CURRENT_DATE(), MAX(i.invoice_date), DAY) AS days_since_last_invoice,

    -- Flags
    MAX(i.invoice_date) >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY) AS is_active_90d

  FROM customers c
  LEFT JOIN invoices i USING (customer_id)
  LEFT JOIN payments p USING (customer_id)

  GROUP BY 1, 2, 3, 4
)

SELECT * FROM customer_metrics
```

### Exemplo de Modelo Mart (Gold)

```sql
-- definitions/marts/revenue_ops/fct_customer_health.sqlx

config {
  type: "table",
  schema: "gold_revenue_ops",
  description: `
    Tabela de Customer Health Score - indicador consolidado de saÃºde do cliente.

    **Owner**: Revenue Operations Team
    **SLA**: D+1 (atualizaÃ§Ã£o diÃ¡ria Ã s 6 AM BRT)
    **Consumidores**: Looker Dashboard "Customer Success Overview"

    **Metodologia Health Score**:
    - 0-40: Churn Risk
    - 41-70: At Risk
    - 71-85: Healthy
    - 86-100: Champion
  `,
  bigquery: {
    partitionBy: "snapshot_date",
    clusterBy: ["health_score_bucket", "customer_segment"]
  },
  tags: ["daily", "revenue_ops", "critical"],

  assertions: {
    nonNull: ["customer_id", "health_score"],
    uniqueKey: ["customer_id", "snapshot_date"]
  }
}

WITH customer_ltv AS (
  SELECT * FROM ${ref("int_customer_lifetime_metrics")}
),

subscription_status AS (
  SELECT
    customer_id,
    MAX(CASE WHEN status = 'active' THEN 1 ELSE 0 END) AS has_active_subscription,
    SUM(monthly_value_brl) AS total_mrr
  FROM ${ref("stg_postgres__subscriptions")}
  GROUP BY 1
),

support_tickets AS (
  SELECT
    customer_id,
    COUNT(*) AS tickets_last_30d,
    AVG(CASE WHEN satisfaction_score IS NOT NULL THEN satisfaction_score END) AS avg_csat
  FROM ${ref("stg_zendesk__tickets")}
  WHERE created_at >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
  GROUP BY 1
),

product_usage AS (
  SELECT
    user_id AS customer_id,
    COUNT(DISTINCT DATE(event_timestamp)) AS active_days_last_30d,
    COUNT(*) AS total_events_last_30d
  FROM ${ref("stg_firebase__events")}
  WHERE event_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
  GROUP BY 1
),

health_calculation AS (
  SELECT
    c.customer_id,
    c.email,
    c.customer_segment,
    CURRENT_DATE() AS snapshot_date,

    -- Componentes do Health Score (0-100)
    CASE
      WHEN s.has_active_subscription = 1 THEN 30
      ELSE 0
    END AS score_subscription,

    LEAST(c.total_revenue_brl / 1000 * 10, 20) AS score_revenue,  -- Max 20 pts

    CASE
      WHEN c.is_active_90d THEN 15
      ELSE 0
    END AS score_recent_activity,

    LEAST(u.active_days_last_30d * 1.5, 20) AS score_product_usage,  -- Max 20 pts

    LEAST(COALESCE(t.avg_csat, 3) * 2.5, 10) AS score_satisfaction,  -- Max 10 pts (CSAT 1-5)

    CASE
      WHEN COALESCE(t.tickets_last_30d, 0) > 5 THEN -5  -- Penalidade
      ELSE 5
    END AS score_support,

    -- Metadados
    c.total_revenue_brl,
    c.total_invoices,
    c.days_since_last_invoice,
    s.total_mrr,
    u.active_days_last_30d,
    t.tickets_last_30d,
    t.avg_csat

  FROM customer_ltv c
  LEFT JOIN subscription_status s USING (customer_id)
  LEFT JOIN support_tickets t USING (customer_id)
  LEFT JOIN product_usage u USING (customer_id)
),

final AS (
  SELECT
    *,
    -- Health Score consolidado
    score_subscription + score_revenue + score_recent_activity +
    score_product_usage + score_satisfaction + score_support AS health_score,

    -- ClassificaÃ§Ã£o
    CASE
      WHEN (score_subscription + score_revenue + score_recent_activity +
            score_product_usage + score_satisfaction + score_support) >= 86 THEN 'Champion'
      WHEN (score_subscription + score_revenue + score_recent_activity +
            score_product_usage + score_satisfaction + score_support) >= 71 THEN 'Healthy'
      WHEN (score_subscription + score_revenue + score_recent_activity +
            score_product_usage + score_satisfaction + score_support) >= 41 THEN 'At Risk'
      ELSE 'Churn Risk'
    END AS health_score_bucket,

    CURRENT_TIMESTAMP() AS _updated_at

  FROM health_calculation
)

SELECT * FROM final
```

### Functions ReutilizÃ¡veis (JavaScript)

```javascript
// includes/helpers.js

// FunÃ§Ã£o para gerar cÃ³digo de deduplicaÃ§Ã£o
function deduplicateByColumn(column, orderBy = 'updated_at') {
  return `
    QUALIFY ROW_NUMBER() OVER (
      PARTITION BY ${column}
      ORDER BY ${orderBy} DESC
    ) = 1
  `;
}

// FunÃ§Ã£o para adicionar metadados padrÃ£o
function addMetadataColumns(sourceSystem) {
  return `
    '${sourceSystem}' AS _source_system,
    CURRENT_TIMESTAMP() AS _loaded_at,
    '${dataform.projectConfig.defaultDatabase}' AS _project,
    '${dataform.projectConfig.defaultSchema}' AS _dataset
  `;
}

// FunÃ§Ã£o para particionamento incremental
function incrementalWhere(timestampColumn) {
  return `
    ${when(incremental(),
      `WHERE ${timestampColumn} > (SELECT MAX(${timestampColumn}) FROM ${self()})`
    )}
  `;
}

module.exports = {
  deduplicateByColumn,
  addMetadataColumns,
  incrementalWhere
};
```

### ConfiguraÃ§Ã£o do Projeto

```json
// dataform.json
{
  "warehouse": "bigquery",
  "defaultProject": "conta-azul-prod",
  "defaultDataset": "silver_core",
  "assertionSchema": "dataform_assertions",
  "defaultLocation": "southamerica-east1",
  "vars": {
    "environment": "production",
    "bronze_project": "conta-azul-prod",
    "silver_project": "conta-azul-prod",
    "gold_project": "conta-azul-prod"
  }
}
```

---

## OrquestraÃ§Ã£o com Cloud Composer

### Arquitetura de OrquestraÃ§Ã£o

```
Cloud Composer (Managed Airflow)
    â†“
DAGs (Python)
    â”œâ”€â”€ daily_ingestion_check.py      # Valida completude Bronze
    â”œâ”€â”€ dataform_silver_layer.py       # Executa modelos staging
    â”œâ”€â”€ dataform_gold_layer.py         # Executa marts
    â”œâ”€â”€ data_quality_checks.py         # Dataplex DQ + custom
    â””â”€â”€ reporting_refresh.py           # Atualiza dashboards
```

### DAG Principal - Pipeline DiÃ¡rio

```python
# dags/daily_data_pipeline.py

from airflow import DAG
from airflow.providers.google.cloud.operators.dataform import (
    DataformCreateCompilationResultOperator,
    DataformCreateWorkflowInvocationOperator,
)
from airflow.providers.google.cloud.operators.bigquery import (
    BigQueryCheckOperator,
    BigQueryExecuteQueryOperator,
)
from airflow.providers.google.cloud.sensors.gcs import GCSObjectsWithPrefixExistenceSensor
from airflow.operators.python import PythonOperator
from airflow.utils.task_group import TaskGroup
from datetime import datetime, timedelta

PROJECT_ID = "conta-azul-prod"
REGION = "southamerica-east1"
DATAFORM_REPO = "dataform-conta-azul"

default_args = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'email': ['data-alerts@contaazul.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
}

with DAG(
    'daily_data_pipeline',
    default_args=default_args,
    description='Pipeline diÃ¡rio completo: Bronze â†’ Silver â†’ Gold',
    schedule_interval='0 3 * * *',  # 3 AM BRT (6 AM UTC)
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['production', 'daily', 'critical'],
    max_active_runs=1,
) as dag:

    # ============================================
    # STEP 1: Validar completude da ingestÃ£o Bronze
    # ============================================
    with TaskGroup('validate_bronze_ingestion') as bronze_validation:

        # Checar se arquivos do Datastream chegaram
        check_postgres_ingestion = GCSObjectsWithPrefixExistenceSensor(
            task_id='check_postgres_files',
            bucket='conta-azul-datalake',
            prefix='bronze/sources/postgres_production/customers/year={{ ds_nodash[:4] }}/month={{ ds_nodash[4:6] }}/day={{ ds_nodash[6:8] }}/',
            timeout=1800,  # 30 min timeout
        )

        # Validar contagem mÃ­nima de registros
        check_bronze_row_count = BigQueryCheckOperator(
            task_id='check_bronze_row_count',
            sql="""
                SELECT COUNT(*) >= 1000  -- Esperamos pelo menos 1k customers
                FROM `conta-azul-prod.bronze_postgres.customers`
                WHERE DATE(_PARTITIONTIME) = '{{ ds }}'
            """,
            use_legacy_sql=False,
        )

        check_postgres_ingestion >> check_bronze_row_count

    # ============================================
    # STEP 2: Executar Dataform - Silver Layer
    # ============================================
    with TaskGroup('dataform_silver_layer') as silver_layer:

        # Compilar modelos Dataform
        compile_silver = DataformCreateCompilationResultOperator(
            task_id='compile_silver_models',
            project_id=PROJECT_ID,
            region=REGION,
            repository_id=DATAFORM_REPO,
            compilation_result={
                'git_commitish': 'main',
                'code_compilation_config': {
                    'vars': {
                        'execution_date': '{{ ds }}',
                        'environment': 'production'
                    }
                }
            }
        )

        # Executar modelos staging (Silver)
        run_silver = DataformCreateWorkflowInvocationOperator(
            task_id='run_silver_models',
            project_id=PROJECT_ID,
            region=REGION,
            repository_id=DATAFORM_REPO,
            workflow_invocation={
                'compilation_result': "{{ task_instance.xcom_pull('compile_silver_models')['name'] }}",
                'invocation_config': {
                    'included_tags': ['daily', 'staging'],
                    'transitive_dependencies': True,
                    'fully_refresh_incremental_tables': False
                }
            }
        )

        compile_silver >> run_silver

    # ============================================
    # STEP 3: Testes de qualidade Silver
    # ============================================
    with TaskGroup('silver_quality_checks') as silver_quality:

        # Dataform assertions (built-in)
        run_silver_tests = DataformCreateWorkflowInvocationOperator(
            task_id='run_silver_assertions',
            project_id=PROJECT_ID,
            region=REGION,
            repository_id=DATAFORM_REPO,
            workflow_invocation={
                'compilation_result': "{{ task_instance.xcom_pull('compile_silver_models')['name'] }}",
                'invocation_config': {
                    'included_tags': ['staging'],
                    'transitive_dependencies': False,
                    'actions_only': False,  # Inclui assertions
                }
            }
        )

        # Dataplex Data Quality (managed)
        run_dataplex_dq = BigQueryExecuteQueryOperator(
            task_id='run_dataplex_quality_checks',
            sql="""
                CALL `conta-azul-prod.dataplex_dq.run_quality_checks`(
                    'silver_core',
                    ARRAY['customers', 'invoices', 'payments']
                )
            """,
            use_legacy_sql=False,
        )

        [run_silver_tests, run_dataplex_dq]

    # ============================================
    # STEP 4: Executar Dataform - Gold Layer (Marts)
    # ============================================
    with TaskGroup('dataform_gold_layer') as gold_layer:

        # Compilar modelos Gold
        compile_gold = DataformCreateCompilationResultOperator(
            task_id='compile_gold_models',
            project_id=PROJECT_ID,
            region=REGION,
            repository_id=DATAFORM_REPO,
            compilation_result={
                'git_commitish': 'main',
            }
        )

        # Executar marts por domÃ­nio (paralelo)
        for domain in ['product_analytics', 'revenue_ops', 'data_science']:
            run_mart = DataformCreateWorkflowInvocationOperator(
                task_id=f'run_{domain}_mart',
                project_id=PROJECT_ID,
                region=REGION,
                repository_id=DATAFORM_REPO,
                workflow_invocation={
                    'compilation_result': "{{ task_instance.xcom_pull('compile_gold_models')['name'] }}",
                    'invocation_config': {
                        'included_tags': [domain],
                        'transitive_dependencies': True,
                    }
                }
            )
            compile_gold >> run_mart

    # ============================================
    # STEP 5: Testes de qualidade Gold
    # ============================================
    test_gold_metrics = BigQueryCheckOperator(
        task_id='test_gold_revenue_positive',
        sql="""
            SELECT
              LOGICAL_AND(mrr_brl >= 0) AS all_positive,
              COUNT(*) > 0 AS has_data
            FROM `conta-azul-prod.gold_revenue_ops.fct_monthly_recurring_revenue`
            WHERE month = DATE_TRUNC('{{ ds }}', MONTH)
        """,
        use_legacy_sql=False,
    )

    # ============================================
    # STEP 6: Refresh Looker Dashboards (PDTs)
    # ============================================
    def trigger_looker_refresh(**context):
        import looker_sdk
        sdk = looker_sdk.init40()

        # Refresh dashboard crÃ­ticos
        dashboard_ids = [123, 456, 789]  # Revenue Ops dashboards
        for dash_id in dashboard_ids:
            sdk.run_look(look_id=dash_id, result_format='json')

        return f"Refreshed {len(dashboard_ids)} dashboards"

    refresh_looker = PythonOperator(
        task_id='refresh_looker_dashboards',
        python_callable=trigger_looker_refresh,
    )

    # ============================================
    # STEP 7: Gerar RelatÃ³rio de Observabilidade
    # ============================================
    def generate_daily_report(**context):
        from google.cloud import bigquery
        import pandas as pd

        client = bigquery.Client()

        # Query estatÃ­sticas do pipeline
        stats_query = """
            SELECT
              table_name,
              row_count,
              size_mb,
              last_modified
            FROM `conta-azul-prod.INFORMATION_SCHEMA.TABLE_STORAGE`
            WHERE table_schema IN ('silver_core', 'gold_revenue_ops')
              AND DATE(last_modified) = CURRENT_DATE()
            ORDER BY size_mb DESC
        """

        df = client.query(stats_query).to_dataframe()

        # Enviar para Slack
        # (implementaÃ§Ã£o omitida para brevidade)

        return "Daily report sent to Slack"

    send_daily_report = PythonOperator(
        task_id='send_observability_report',
        python_callable=generate_daily_report,
    )

    # ============================================
    # STEP 8: Update Metadata Catalog
    # ============================================
    update_catalog = BigQueryExecuteQueryOperator(
        task_id='update_data_catalog',
        sql="""
            -- Atualizar estatÃ­sticas de freshness
            MERGE `conta-azul-prod.metadata.table_freshness` T
            USING (
              SELECT
                CONCAT(table_schema, '.', table_name) AS full_table_name,
                MAX(TIMESTAMP_MILLIS(creation_time)) AS last_updated
              FROM `conta-azul-prod.INFORMATION_SCHEMA.TABLES`
              WHERE table_schema LIKE 'gold_%'
              GROUP BY 1
            ) S
            ON T.table_name = S.full_table_name
            WHEN MATCHED THEN
              UPDATE SET last_updated = S.last_updated
            WHEN NOT MATCHED THEN
              INSERT (table_name, last_updated) VALUES (S.full_table_name, S.last_updated)
        """,
        use_legacy_sql=False,
    )

    # ============================================
    # DependÃªncias do Pipeline
    # ============================================
    bronze_validation >> silver_layer >> silver_quality
    silver_quality >> gold_layer >> test_gold_metrics
    test_gold_metrics >> [refresh_looker, update_catalog]
    [refresh_looker, update_catalog] >> send_daily_report
```

### Monitoramento do Airflow

```python
# dags/monitoring/pipeline_sla_monitor.py

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from datetime import datetime, timedelta

def check_pipeline_sla(**context):
    """
    Verifica se tabelas Gold foram atualizadas dentro do SLA (D+1 atÃ© 6 AM)
    """
    hook = BigQueryHook()

    query = """
        SELECT
          table_name,
          TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), last_modified, HOUR) AS hours_since_update
        FROM `conta-azul-prod.gold_revenue_ops.INFORMATION_SCHEMA.TABLES`
        WHERE table_type = 'BASE TABLE'
    """

    df = hook.get_pandas_df(query)

    # SLA: mÃ¡ximo 27 horas (D+1 + 3h de buffer)
    sla_breaches = df[df['hours_since_update'] > 27]

    if not sla_breaches.empty:
        # Enviar alerta crÃ­tico
        send_slack_alert(
            channel='#data-alerts',
            message=f"ðŸš¨ SLA BREACH: {len(sla_breaches)} tables estÃ£o desatualizadas"
        )
        raise Exception(f"SLA breach detected: {sla_breaches.to_dict()}")

    return "All tables within SLA"

with DAG(
    'pipeline_sla_monitor',
    schedule_interval='0 7 * * *',  # 7 AM BRT (1h apÃ³s deadline)
    start_date=datetime(2025, 1, 1),
    catchup=False,
) as dag:

    check_sla = PythonOperator(
        task_id='check_gold_layer_sla',
        python_callable=check_pipeline_sla,
    )
```

---

## Qualidade e GovernanÃ§a

### 1. Versionamento com Git (Cloud Source Repositories)

**RepositÃ³rios**:
```
Cloud Source Repositories (ou GitHub)
â”œâ”€â”€ dataform-conta-azul/          # Modelos Dataform
â”œâ”€â”€ airflow-dags/                  # DAGs do Composer
â”œâ”€â”€ ingestion-scripts/             # Cloud Run/Functions custom
â””â”€â”€ data-quality-rules/            # Dataplex DQ configs
```

**Branching Strategy**:
```
main (produÃ§Ã£o)
â”œâ”€â”€ staging (homologaÃ§Ã£o)
â””â”€â”€ feature/* (desenvolvimento)
```

**Pull Request Workflow**:
1. Developer cria branch `feature/add-churn-model`
2. Desenvolve modelo + testes
3. Abre PR para `staging`
4. CI/CD executa:
   - `dataform compile` (syntax check)
   - `dataform run --dry-run` (impacto estimado)
   - Testes unitÃ¡rios
5. Code review por Senior Analytics Engineer
6. Merge para `staging` â†’ deploy automÃ¡tico para env de staging
7. Testes E2E em staging
8. PR de `staging` para `main` â†’ deploy produÃ§Ã£o

### 2. CI/CD com Cloud Build

```yaml
# cloudbuild.yaml

steps:
  # Step 1: Validar sintaxe SQL
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'dataform-compile'
    args:
      - 'beta'
      - 'dataform'
      - 'compile'
      - '--project=conta-azul-prod'
      - '--region=southamerica-east1'
      - '--repository=dataform-conta-azul'
      - '--git-commitish=$BRANCH_NAME'

  # Step 2: Executar dry-run (estimar impacto)
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'dataform-dry-run'
    args:
      - 'beta'
      - 'dataform'
      - 'run'
      - '--project=conta-azul-prod'
      - '--dry-run'
      - '--compilation-result=compilations/latest'

  # Step 3: Executar testes customizados
  - name: 'python:3.11'
    id: 'run-custom-tests'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        pip install -q pytest google-cloud-bigquery pandas
        pytest tests/ -v

  # Step 4: Deploy para staging (se branch = staging)
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'deploy-staging'
    args:
      - 'beta'
      - 'dataform'
      - 'run'
      - '--project=conta-azul-staging'
      - '--compilation-result=compilations/latest'
      - '--tags=staging'
    waitFor: ['dataform-compile', 'run-custom-tests']
    env:
      - 'BRANCH=$BRANCH_NAME'

# Trigger apenas em branches especÃ­ficas
options:
  logging: CLOUD_LOGGING_ONLY
  machineType: 'N1_HIGHCPU_8'

# NotificaÃ§Ãµes
substitutions:
  _SLACK_WEBHOOK: 'https://hooks.slack.com/services/...'

# Executar apenas em PRs e merges
trigger:
  branches:
    - ^main$
    - ^staging$
    - ^feature/.*
```

### 3. Testes de Qualidade de Dados

**NÃ­vel 1: Dataform Assertions (Inline)**

```sql
-- definitions/staging/core/stg_postgres__customers.sqlx

config {
  assertions: {
    uniqueKey: ["customer_id"],
    nonNull: ["customer_id", "email", "created_date"],
    rowConditions: [
      "email LIKE '%@%'",  -- Email vÃ¡lido
      "created_date <= CURRENT_DATE()",  -- NÃ£o futuro
      "customer_segment IN ('PME', 'MEI', 'AutÃ´nomo', 'Outros')"
    ]
  }
}
```

**NÃ­vel 2: Dataplex Data Quality (Managed)**

```yaml
# dataplex_dq/rules/customers_quality.yaml

entity: silver_core.customers
row_filters: "created_date >= CURRENT_DATE() - 365"  # Ãšltimos 12 meses

rules:
  # Completude
  - column: customer_id
    dimension: COMPLETENESS
    threshold: 100%

  - column: email
    dimension: COMPLETENESS
    threshold: 99.5%  # 0.5% de tolerÃ¢ncia

  # Unicidade
  - column: customer_id
    dimension: UNIQUENESS
    threshold: 100%

  # Validade
  - column: email
    dimension: VALIDITY
    rule_type: REGEX
    regex: '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    threshold: 99%

  - column: customer_segment
    dimension: VALIDITY
    rule_type: SET_EXPECTATION
    expected_values: ['PME', 'MEI', 'AutÃ´nomo', 'Outros']
    threshold: 100%

  # ConsistÃªncia
  - dimension: CONSISTENCY
    rule_type: CUSTOM_SQL
    sql: |
      SELECT COUNT(*)
      FROM silver_core.customers c
      LEFT JOIN silver_core.subscriptions s USING (customer_id)
      WHERE c.customer_segment = 'PME'
        AND s.plan_type = 'free'  -- InconsistÃªncia: PME nÃ£o deve ter plano free
    threshold: 0  # Zero ocorrÃªncias

  # Atualidade (Freshness)
  - dimension: TIMELINESS
    rule_type: CUSTOM_SQL
    sql: |
      SELECT TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), MAX(_loaded_at), HOUR)
      FROM silver_core.customers
    threshold: 24  # MÃ¡ximo 24 horas desatualizado
```

**NÃ­vel 3: Testes Customizados (Python + pytest)**

```python
# tests/test_revenue_ops_mart.py

import pytest
from google.cloud import bigquery
import pandas as pd
from datetime import datetime, timedelta

@pytest.fixture
def bq_client():
    return bigquery.Client(project='conta-azul-prod')

def test_mrr_always_positive(bq_client):
    """MRR nunca deve ser negativo"""
    query = """
        SELECT COUNT(*) as negative_count
        FROM `conta-azul-prod.gold_revenue_ops.fct_monthly_recurring_revenue`
        WHERE mrr_brl < 0
    """
    df = bq_client.query(query).to_dataframe()
    assert df['negative_count'][0] == 0, "Found negative MRR values"

def test_mrr_growth_plausible(bq_client):
    """Crescimento de MRR nÃ£o deve ser > 50% MoM (anomalia)"""
    query = """
        WITH growth AS (
          SELECT
            month,
            mrr_brl,
            LAG(mrr_brl) OVER (ORDER BY month) AS prev_mrr,
            (mrr_brl - LAG(mrr_brl) OVER (ORDER BY month)) /
              LAG(mrr_brl) OVER (ORDER BY month) AS growth_rate
          FROM `conta-azul-prod.gold_revenue_ops.fct_monthly_recurring_revenue`
          WHERE customer_segment = 'PME'
        )
        SELECT MAX(ABS(growth_rate)) AS max_growth
        FROM growth
        WHERE month >= DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH)
    """
    df = bq_client.query(query).to_dataframe()
    assert df['max_growth'][0] < 0.5, "MRR growth > 50% detected (potential anomaly)"

def test_customer_health_score_range(bq_client):
    """Health score deve estar entre 0-100"""
    query = """
        SELECT
          MIN(health_score) AS min_score,
          MAX(health_score) AS max_score
        FROM `conta-azul-prod.gold_revenue_ops.fct_customer_health`
        WHERE snapshot_date = CURRENT_DATE()
    """
    df = bq_client.query(query).to_dataframe()
    assert 0 <= df['min_score'][0] <= 100
    assert 0 <= df['max_score'][0] <= 100

def test_data_freshness_sla(bq_client):
    """Gold tables devem ser atualizados D+1 (mÃ¡ximo 27h)"""
    query = """
        SELECT
          table_name,
          TIMESTAMP_DIFF(CURRENT_TIMESTAMP(),
                         TIMESTAMP_MILLIS(last_modified_time), HOUR) AS hours_old
        FROM `conta-azul-prod.gold_revenue_ops.__TABLES__`
        WHERE table_id NOT LIKE '%_temp%'
    """
    df = bq_client.query(query).to_dataframe()
    stale_tables = df[df['hours_old'] > 27]

    assert stale_tables.empty, f"Stale tables found: {stale_tables['table_name'].tolist()}"
```

### 4. Observabilidade e Monitoramento

**Cloud Monitoring Dashboards**:
```
Dashboard: Data Pipeline Health
â”œâ”€â”€ Painel 1: Freshness
â”‚   â”œâ”€â”€ "Time since last update" (por table)
â”‚   â””â”€â”€ Alerta: > 27 horas
â”œâ”€â”€ Painel 2: Data Quality
â”‚   â”œâ”€â”€ "Dataplex DQ pass rate" (Ãºltimas 24h)
â”‚   â””â”€â”€ "Dataform assertion failures" (count)
â”œâ”€â”€ Painel 3: Performance
â”‚   â”œâ”€â”€ "BigQuery slot usage" (timeseries)
â”‚   â”œâ”€â”€ "Query execution time" (p50, p95, p99)
â”‚   â””â”€â”€ "Bytes processed/scanned" (cost proxy)
â””â”€â”€ Painel 4: Pipeline Status
    â”œâ”€â”€ "Airflow DAG success rate" (Ãºltimos 7 dias)
    â””â”€â”€ "Failed task count" (por DAG)
```

**Alertas CrÃ­ticos** (Cloud Monitoring + Slack):
```yaml
alerts:
  - name: "Gold Layer SLA Breach"
    condition: "table_freshness > 27 hours"
    severity: CRITICAL
    notification: ["#data-alerts", "data-eng-oncall@contaazul.com"]

  - name: "Data Quality Failure"
    condition: "dataplex_dq_pass_rate < 95%"
    severity: HIGH
    notification: ["#data-alerts"]

  - name: "BigQuery Cost Spike"
    condition: "daily_cost > $500 OR daily_cost > 2x(7d_avg)"
    severity: MEDIUM
    notification: ["#data-eng", "finops@contaazul.com"]

  - name: "Airflow DAG Failure"
    condition: "dag_failure AND dag_id = 'daily_data_pipeline'"
    severity: CRITICAL
    notification: ["#data-alerts", "pagerduty"]
```

### 5. GovernanÃ§a de Dados

**Dataplex Data Catalog** (Auto-discovery + Manual enrichment):

```
CatÃ¡logo de Dados
â”œâ”€â”€ DomÃ­nio: Product Analytics
â”‚   â”œâ”€â”€ Dataset: gold_product_analytics.fct_user_engagement
â”‚   â”‚   â”œâ”€â”€ DescriÃ§Ã£o: "Engajamento de usuÃ¡rios (DAU, WAU, MAU)"
â”‚   â”‚   â”œâ”€â”€ Owner: product-analytics@contaazul.com
â”‚   â”‚   â”œâ”€â”€ Tags: [pii, daily, critical]
â”‚   â”‚   â”œâ”€â”€ SLA: D+1 6 AM BRT
â”‚   â”‚   â””â”€â”€ Consumers: ["Looker Dashboard: Product Overview"]
â”‚   â””â”€â”€ ...
â”œâ”€â”€ DomÃ­nio: Revenue Operations
â””â”€â”€ DomÃ­nio: Data Science
```

**Data Classification (DLP)**:
```sql
-- Aplicar tags de sensibilidade automaticamente
-- Policy Tag Taxonomy: conta-azul-prod.data_classification

CREATE POLICY TAG sensitive_pii
  ON `conta-azul-prod.data_classification`
  WITH DESCRIPTION 'Dados pessoais sensÃ­veis (LGPD Art. 5Âº)'
  AND masking_rule = 'SHA256';  -- Masking para nÃ£o-autorizados

-- Aplicar em colunas
ALTER TABLE `conta-azul-prod.silver_core.customers`
  SET COLUMN OPTIONS (
    email = (description = 'Email do cliente',
             policy_tags = ['projects/conta-azul-prod/locations/southamerica-east1/taxonomies/123/policyTags/sensitive_pii'])
  );
```

**Row-Level Security (RLS)**:
```sql
-- Analistas de RevOps sÃ³ veem clientes do segmento PME
CREATE ROW ACCESS POLICY revops_only_pme
  ON `conta-azul-prod.gold_revenue_ops.fct_customer_health`
  GRANT TO ('group:revops-analysts@contaazul.com')
  FILTER USING (customer_segment = 'PME');

-- Data Science team vÃª todos os dados (anonimizados)
CREATE ROW ACCESS POLICY data_science_all
  ON `conta-azul-prod.gold_data_science.features_customer_churn`
  GRANT TO ('group:data-science@contaazul.com')
  FILTER USING (TRUE);
```

---

## Camada de Consumo

### 1. Para Analistas de Dados (SQL Users)

**BigQuery Studio** (interface SQL nativa):
- Query editor com autocomplete
- Saved queries e notebooks SQL
- Compartilhamento de queries via link

**Looker / Looker Studio**:

**Exemplo de Dashboard (Revenue Operations)**:
```
Dashboard: Revenue Operations Overview
â”œâ”€â”€ KPI Cards (Top)
â”‚   â”œâ”€â”€ MRR Atual: R$ 1.2M (+8% MoM)
â”‚   â”œâ”€â”€ Churn Rate: 3.2% (â†“ 0.5pp)
â”‚   â”œâ”€â”€ Customer Health Score MÃ©dio: 78/100
â”‚   â””â”€â”€ Active Subscriptions: 3,456 (+120 MoM)
â”œâ”€â”€ Charts (Middle)
â”‚   â”œâ”€â”€ [Line] MRR Evolution (12 meses)
â”‚   â”œâ”€â”€ [Funnel] Sales Pipeline by Stage
â”‚   â”œâ”€â”€ [Bar] MRR by Customer Segment
â”‚   â””â”€â”€ [Heatmap] Customer Health Score Distribution
â””â”€â”€ Table (Bottom)
    â””â”€â”€ Top 50 Customers at Churn Risk (Health < 40)
```

**LookML (Looker semantic layer)**:
```lookml
# models/revenue_ops.model.lkml

connection: "bigquery_conta_azul_prod"

include: "/views/**/*.view.lkml"

explore: fct_monthly_recurring_revenue {
  label: "Monthly Recurring Revenue"

  join: dim_customers {
    sql_on: ${fct_monthly_recurring_revenue.customer_id} = ${dim_customers.customer_id} ;;
    relationship: many_to_one
  }

  always_filter: {
    filters: [fct_monthly_recurring_revenue.month: "12 months"]
  }
}

# views/fct_monthly_recurring_revenue.view.lkml

view: fct_monthly_recurring_revenue {
  sql_table_name: `conta-azul-prod.gold_revenue_ops.fct_monthly_recurring_revenue` ;;

  dimension_group: month {
    type: time
    timeframes: [date, month, quarter, year]
    sql: ${TABLE}.month ;;
  }

  dimension: customer_segment {
    type: string
    sql: ${TABLE}.customer_segment ;;
  }

  measure: total_mrr {
    type: sum
    sql: ${TABLE}.mrr_brl ;;
    value_format_name: brl_currency
    label: "Total MRR (R$)"
  }

  measure: mrr_growth_pct {
    type: number
    sql:
      (${total_mrr} - LAG(${total_mrr}) OVER (ORDER BY ${month_month})) /
      LAG(${total_mrr}) OVER (ORDER BY ${month_month})
    ;;
    value_format_name: percent_1
  }
}
```

### 2. Para Cientistas de Dados (Python/R/Notebooks)

**Vertex AI Workbench** (Managed Jupyter):

```python
# notebook: customer_churn_prediction.ipynb

# Conectar ao BigQuery
from google.cloud import bigquery
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

client = bigquery.Client(project='conta-azul-prod')

# Ler feature store
query = """
    SELECT *
    FROM `conta-azul-prod.gold_data_science.features_customer_churn`
    WHERE snapshot_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH)
"""

df = client.query(query).to_dataframe()

# Features disponÃ­veis (jÃ¡ preparadas pelo Analytics Engineering):
# - customer_lifetime_days
# - total_revenue_brl
# - days_since_last_login
# - feature_usage_score
# - support_tickets_count_30d
# - payment_failure_count
# - health_score
# - has_active_subscription
# - churn_label (target)

X = df.drop(['customer_id', 'snapshot_date', 'churn_label'], axis=1)
y = df['churn_label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinar modelo
model = RandomForestClassifier(n_estimators=100, max_depth=10)
model.fit(X_train, y_train)

# Avaliar
from sklearn.metrics import classification_report, roc_auc_score
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

print(f"ROC AUC: {roc_auc_score(y_test, y_proba):.3f}")
print(classification_report(y_test, y_pred))

# Salvar previsÃµes de volta no BigQuery
predictions_df = df[['customer_id']].copy()
predictions_df['churn_probability'] = model.predict_proba(X)[:, 1]
predictions_df['prediction_date'] = pd.Timestamp.now()

# Write to BigQuery
predictions_df.to_gbq(
    destination_table='gold_data_science.customer_churn_predictions',
    project_id='conta-azul-prod',
    if_exists='append'
)
```

**BigQuery ML** (ML direto no SQL - sem Python):

```sql
-- Treinar modelo de churn dentro do BigQuery
CREATE OR REPLACE MODEL `conta-azul-prod.gold_data_science.churn_model_bqml`
OPTIONS(
  model_type='LOGISTIC_REG',
  input_label_cols=['churn_label'],
  auto_class_weights=TRUE,
  max_iterations=50
) AS
SELECT
  -- Features
  customer_lifetime_days,
  total_revenue_brl,
  days_since_last_login,
  feature_usage_score,
  support_tickets_count_30d,
  payment_failure_count,
  health_score,
  has_active_subscription,

  -- Target
  churn_label
FROM `conta-azul-prod.gold_data_science.features_customer_churn`
WHERE snapshot_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 12 MONTH);

-- Avaliar modelo
SELECT
  roc_auc,
  precision,
  recall
FROM ML.EVALUATE(
  MODEL `conta-azul-prod.gold_data_science.churn_model_bqml`,
  (
    SELECT * FROM `conta-azul-prod.gold_data_science.features_customer_churn`
    WHERE snapshot_date = CURRENT_DATE()
  )
);

-- Fazer previsÃµes
SELECT
  customer_id,
  predicted_churn_label,
  predicted_churn_label_probs[OFFSET(1)].prob AS churn_probability
FROM ML.PREDICT(
  MODEL `conta-azul-prod.gold_data_science.churn_model_bqml`,
  (SELECT * FROM `conta-azul-prod.gold_data_science.features_customer_churn`
   WHERE snapshot_date = CURRENT_DATE())
)
ORDER BY churn_probability DESC
LIMIT 100;
```

### 3. Para Stakeholders de NegÃ³cio

**Dashboards Executivos** (Looker - auto-refresh):

```
Dashboard: Executive Summary (CEO/CFO)
â”œâ”€â”€ Revenue Metrics
â”‚   â”œâ”€â”€ MRR: R$ 1.2M (+8% MoM)
â”‚   â”œâ”€â”€ ARR: R$ 14.4M
â”‚   â”œâ”€â”€ Net Revenue Retention: 108%
â”‚   â””â”€â”€ Customer LTV: R$ 4.2k
â”œâ”€â”€ Growth Metrics
â”‚   â”œâ”€â”€ New Customers (month): 120
â”‚   â”œâ”€â”€ Churned Customers: 38 (-3.2%)
â”‚   â”œâ”€â”€ Expansion Revenue: R$ 45k
â”‚   â””â”€â”€ Sales Pipeline: R$ 3.1M
â”œâ”€â”€ Product Metrics
â”‚   â”œâ”€â”€ MAU (Monthly Active Users): 12.4k (+15%)
â”‚   â”œâ”€â”€ Feature Adoption Rate: 68%
â”‚   â””â”€â”€ NPS: 72 (Promoters - Detractors)
â””â”€â”€ [Line Chart] 12-month Revenue & User Growth Trend
```

**Scheduled Reports** (Email/Slack):
```python
# ConfiguraÃ§Ã£o Looker Schedule
schedule:
  frequency: daily
  time: "08:00 BRT"
  recipients:
    - ceo@contaazul.com
    - cfo@contaazul.com
    - "Slack: #executive-updates"
  format: PDF
  filters:
    date_range: "yesterday"
```

**Data Catalog Self-Service** (Dataplex):
- Busca por keyword: "customer churn"
- Visualiza: datasets, columns, lineage, owners
- Solicita acesso via IAM (self-service approval)

---

## Estimativa de Custos (Mensais)

### Premissas
- **Volume de dados**: ~100 GB novos/mÃªs, 1 TB total armazenado
- **Eventos**: ~500k eventos/dia (product analytics)
- **Queries**: ~2 TB processados/mÃªs (anÃ¡lises + dashboards)
- **UsuÃ¡rios**: 50 analistas/cientistas + 20 dashboards Looker

### Breakdown de Custos (USD)

| ServiÃ§o GCP | DescriÃ§Ã£o | Custo Mensal |
|-------------|-----------|--------------|
| **Cloud Storage** | 1 TB (Standard class) | $20 |
| **BigQuery Storage** | 1 TB active storage + 500 GB long-term | $25 |
| **BigQuery Compute** | 2 TB queries/mÃªs (~$5/TB) | $10 |
| **BigQuery Streaming** | 500k eventos/dia * 30 = 15M rows ($0.01/200MB) | $15 |
| **Datastream** | CDC PostgreSQL (1 stream) | $100 |
| **Fivetran** | 100k MAR * $1.20 | $120 |
| **Cloud Composer** | Small environment (1 vCPU) | $200 |
| **Dataform** | IncluÃ­do no BigQuery | $0 |
| **Dataplex** | 50 tables * $1/table | $50 |
| **Cloud Run** | Custom ingestion (1M requests) | $10 |
| **Looker Studio** | GrÃ¡tis (atÃ© 10 users) | $0 |
| **Vertex AI Notebooks** | 2 instÃ¢ncias n1-standard-4 (8h/dia) | $80 |
| **Cloud Monitoring** | Logs + mÃ©tricas | $20 |
| **Networking** | Egress + inter-region | $30 |
| **Outros** | Cloud Functions, Pub/Sub, etc. | $20 |
| **TOTAL** | | **~$700/mÃªs** |

### OtimizaÃ§Ãµes de Custo

1. **Particionamento e Clustering**: Reduz scans em 80-90%
   ```sql
   -- Sem particionamento: 1 TB scanned = $5
   -- Com particionamento: 100 GB scanned = $0.50
   ```

2. **BigQuery BI Engine** (cache): $40/mÃªs para 10 GB cache â†’ queries repetitivas grÃ¡tis

3. **Cloud Storage Lifecycle**:
   ```yaml
   lifecycle:
     - action: SetStorageClass
       storage_class: NEARLINE
       condition:
         age: 90  # Mover para Nearline apÃ³s 90 dias (75% mais barato)
     - action: Delete
       condition:
         age: 730  # Deletar apÃ³s 2 anos
   ```

4. **Dataform scheduled runs**: Executar apenas modelos modificados (incremental)

5. **Looker PDT persistence**: Cachear queries pesadas (regenerar 1x/dia)

---

## Roadmap de ImplementaÃ§Ã£o

### Fase 0: PreparaÃ§Ã£o (Semana 0)
- [ ] Criar projeto GCP (`conta-azul-prod`)
- [ ] Configurar billing account e budgets/alerts
- [ ] Setup IAM roles e service accounts
- [ ] Criar buckets Cloud Storage (Bronze/Silver)
- [ ] Provisionar BigQuery datasets (bronze_*, silver_*, gold_*)

### Fase 1: Foundation (Semanas 1-2)
- [ ] Deploy Cloud Composer (Airflow) - ambiente Small
- [ ] Setup Cloud Source Repositories para cÃ³digo
- [ ] Configurar CI/CD bÃ¡sico (Cloud Build)
- [ ] Criar estrutura inicial Dataform
- [ ] Deploy Dataplex para catalogaÃ§Ã£o

### Fase 2: IngestÃ£o - Quick Win (Semanas 3-4)
**Objetivo**: 1 fonte de dados end-to-end

- [ ] Conectar PostgreSQL produÃ§Ã£o via Datastream (CDC)
- [ ] Tabelas prioritÃ¡rias: customers, invoices, payments, subscriptions
- [ ] Validar chegada de dados no Bronze (Cloud Storage)
- [ ] Criar BigLake external tables

### Fase 3: TransformaÃ§Ã£o - Silver Layer (Semanas 5-6)
- [ ] Dataform: modelos staging para tabelas do PostgreSQL
- [ ] Implementar deduplicaÃ§Ã£o e limpeza
- [ ] Testes de qualidade bÃ¡sicos (not null, unique)
- [ ] DAG Airflow para orquestraÃ§Ã£o Silver

### Fase 4: TransformaÃ§Ã£o - Gold Layer (Semana 7)
- [ ] Dataform: primeiro mart (`fct_monthly_recurring_revenue`)
- [ ] Modelos dimensionais (dim_customers)
- [ ] DocumentaÃ§Ã£o inline (descriptions, owners)
- [ ] Testes de qualidade avanÃ§ados

### Fase 5: Consumo - BI (Semana 8)
- [ ] Deploy Looker ou Looker Studio
- [ ] Criar LookML views para Gold layer
- [ ] Primeiro dashboard (Revenue Ops Overview)
- [ ] Treinamento para analistas

### Fase 6: Qualidade & Observabilidade (Semana 9)
- [ ] Configurar Dataplex Data Quality rules
- [ ] Implementar testes customizados (pytest)
- [ ] Cloud Monitoring dashboards
- [ ] Alertas crÃ­ticos (SLA, DQ failures)

### Fase 7: ExpansÃ£o de Fontes (Semanas 10-11)
- [ ] Adicionar Firebase Analytics export
- [ ] Conectar Salesforce via Fivetran
- [ ] Google Analytics 4 â†’ BigQuery
- [ ] Normalizar para Silver layer

### Fase 8: Product Analytics & Data Science (Semana 12)
- [ ] Marts de Product Analytics (fct_user_engagement)
- [ ] Feature store para Data Science
- [ ] Deploy Vertex AI Workbench
- [ ] Primeiro modelo BigQuery ML (churn prediction)

### Fase 9: GovernanÃ§a (Semana 13)
- [ ] Implementar Data Catalog completo
- [ ] DLP para classificaÃ§Ã£o de PII
- [ ] Row-Level Security (RLS) por domÃ­nio
- [ ] PolÃ­ticas de retenÃ§Ã£o automatizadas

### Fase 10: OtimizaÃ§Ã£o & Scale (Semanas 14-16)
- [ ] Performance tuning (clustering, BI Engine)
- [ ] Cost optimization audit
- [ ] DocumentaÃ§Ã£o completa (Confluence/Notion)
- [ ] Handoff para time de Analytics Engineering

---

## Diferenciais da SoluÃ§Ã£o

### 1. Serverless-First
- **Zero gerenciamento de infraestrutura**: BigQuery, Dataform, Cloud Run
- **Escala automÃ¡tica**: De 0 a milhÃµes de queries sem reconfiguraÃ§Ã£o
- **Custo variÃ¡vel**: Paga apenas pelo uso real

### 2. Data Mesh Ready
- **DomÃ­nios claros**: Product Analytics, RevOps, Data Science
- **Ownership distribuÃ­do**: Cada time mantÃ©m seus marts
- **Self-service**: Analistas criam novos modelos sem depender de engenharia

### 3. Qualidade como CÃ³digo
- **Testes versionados**: Git + CI/CD
- **MÃºltiplas camadas**: Dataform assertions + Dataplex DQ + pytest
- **AutomaÃ§Ã£o**: Falhas bloqueiam deploy

### 4. Observabilidade Completa
- **Lineage automÃ¡tico**: Dataplex rastreia origem dos dados
- **Freshness tracking**: SLAs monitorados 24/7
- **Cost monitoring**: Alertas de custo anÃ´malo

---

## PrÃ³ximos Passos

1. **ValidaÃ§Ã£o da Proposta**
   - Apresentar arquitetura para stakeholders tÃ©cnicos (CTO, Head of Data)
   - Alinhar prioridades de fontes de dados com Product Analytics, RevOps e Data Science

2. **PriorizaÃ§Ã£o de Use Cases**
   - Identificar "quick win": dashboard mais crÃ­tico para negÃ³cio
   - Exemplo: MRR Dashboard â†’ impacto direto em RevOps

3. **Estimativa Detalhada**
   - Refinar custos com volume real de dados
   - Sizing de Cloud Composer (pode comeÃ§ar menor)

4. **POC (Proof of Concept) - 2 semanas**
   - Fonte: PostgreSQL produÃ§Ã£o
   - Pipeline: Bronze â†’ Silver â†’ Gold (1 mart)
   - Consumo: 1 dashboard Looker
   - **Meta**: Demonstrar valor end-to-end

5. **FormaÃ§Ã£o de Time**
   - 1 Senior Analytics Engineer (arquitetura + mentoria)
   - 2 Analytics Engineers (desenvolvimento)
   - Suporte: 1 Data Engineer (ingestÃ£o complexa)

---

## ApÃªndice

### GlossÃ¡rio de Termos

- **ELT**: Extract, Load, Transform (vs ETL - transformaÃ§Ã£o apÃ³s load)
- **CDC**: Change Data Capture (captura mudanÃ§as em tempo real)
- **Medallion Architecture**: Bronze (raw) â†’ Silver (clean) â†’ Gold (business)
- **Data Mesh**: Arquitetura descentralizada por domÃ­nios de negÃ³cio
- **SLA**: Service Level Agreement (acordo de nÃ­vel de serviÃ§o)
- **MAR**: Monthly Active Rows (mÃ©trica de custo Fivetran)
- **MRR**: Monthly Recurring Revenue
- **DAU/WAU/MAU**: Daily/Weekly/Monthly Active Users

### ReferÃªncias

- [Google Cloud Data Analytics Reference Patterns](https://cloud.google.com/architecture/reference-patterns/overview)
- [Dataform Best Practices](https://cloud.google.com/dataform/docs/best-practices)
- [Medallion Architecture (Databricks)](https://www.databricks.com/glossary/medallion-architecture)
- [Data Mesh Principles](https://www.datamesh-architecture.com/)
- [BigQuery Optimization Guide](https://cloud.google.com/bigquery/docs/best-practices-performance-overview)

### Contato

**Proposta elaborada para**: Conta Azul
**PosiÃ§Ã£o**: Analytics Engineer (Especialista)
**Data**: Outubro 2025
**Plataforma**: Google Cloud Platform (GCP)

---

**ObservaÃ§Ã£o**: Esta Ã© uma proposta de arquitetura (desenho da soluÃ§Ã£o). A implementaÃ§Ã£o completa seguiria o roadmap de 16 semanas descrito acima.
